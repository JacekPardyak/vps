---
title: "VPS Customer churn prediction - Part 2"
author: "JG Pardyak"
date: "02-9-2021"
output:
  beamer_presentation:
    slide_level: 2
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

## Motivation

This is the continuation of the presentations:

- https://github.com/JacekPardyak/vps/blob/master/vps-part-1.pdf ,

- https://github.com/JacekPardyak/vps/blob/master/vps-part-2.pdf .

In this presentation we check:

- outliers - data points that differs significantly from other observations,

- Feature engineering - combining and transforming further existing variables,

- Model tuning of `decision_tree` and `rand_forest` 

# Outlier detection

## **disk_ops** variables 

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
library(tidymodels)

vps <- read_csv("./data/vps_churn_data.txt") %>%
  mutate(is_churn = factor(ifelse(is_churn == 0, 
                                  "No", "Yes")))

vars <- vps %>% names()

disk_ops <- vars[agrep('disk_ops', vars)] # 6
disk_octets <- vars[agrep('disk_octets', vars)] # 6
cpu <- vars[agrep('cpu_', vars)] # 3
network <- vars[agrep('network', vars)] # 6

vps_longer <- vps %>%
  pivot_longer(., cols = !one_of('id', 'is_churn'), names_to = "Variable", values_to = "Value")

vps_longer  %>% filter(Variable %in% disk_ops) %>%
    ggplot(aes(x = Variable, y = Value, fill = is_churn)) +
    geom_boxplot(outlier.colour = "red")
```


##  **disk_octets** variables 

```{r, echo=FALSE}
vps_longer  %>% filter(Variable %in% disk_octets) %>%
  ggplot(aes(x = Variable, y = Value, fill = is_churn)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 8, outlier.size = 4)

```

##  **cpu** variables 

```{r, echo=FALSE}
vps_longer  %>% filter(Variable %in% cpu) %>%
  ggplot(aes(x = Variable, y = Value, fill = is_churn)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 8, outlier.size = 4)
```

## **network** variables 

```{r, echo=FALSE}
vps_longer  %>% filter(Variable %in% network) %>%
  ggplot(aes(x = Variable, y = Value, fill = is_churn)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 8, outlier.size = 4)
```

# Tune `decision_tree` hyperparameters

## Split and prepare data


```{r, echo=TRUE}
set.seed(123)
vps_split <- initial_split(vps, strata = is_churn)
vps_train <- training(vps_split)
vps_test  <- testing(vps_split)
vps_recipe <- vps_train %>%
  recipe(is_churn ~ .) %>% 
  step_rm(id) %>% 
  step_corr(all_predictors()) #%>% 
  # make vars to be of mean zero
#  step_center(all_predictors(), -all_outcomes()) %>%
  # make vars to be standard dev of 1
  #step_scale(all_predictors(), -all_outcomes())
```


## Create model specification for tuning

```{r, echo=TRUE}
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

tune_spec 
```



## Create grid of hyperparameters values 

```{r, echo=TRUE}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
tree_grid
```

## Create cross-validation folds for tuning

```{r, echo=TRUE}
set.seed(234)
vps_folds <- vfold_cv(vps_train)
vps_folds
```

## Fit models at all the different values

```{r, echo=TRUE}
set.seed(345)
tune_wf <- workflow() %>%
  add_model(tune_spec) %>% 
  add_recipe(vps_recipe)
tree_res <- 
  tune_wf %>% 
  tune_grid(
    resamples = vps_folds,
    grid = tree_grid
    )

tree_res
```

## Get metrics of different models

```{r}
tree_res %>% 
  collect_metrics()
```
## Plot metrics of different models

```{r}
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

## Show models with best metrics

```{r}
tree_res %>%
  show_best("accuracy")
```

## Pick one model with the best metrics

```{r, echo=TRUE}
best_tree <- tree_res %>%
  select_best("accuracy")
best_tree
```

## Finalizing best model

```{r, echo=TRUE}
final_wf <- 
  tune_wf %>% 
  finalize_workflow(best_tree)

final_wf
```
## The final fit

```{r, echo=TRUE}
final_fit <- 
  final_wf %>%
  last_fit(vps_split) 

final_fit %>%
  collect_metrics()

```

## The final fit  ROC curve

```{r}
final_fit %>%
  collect_predictions() %>% 
  roc_curve(is_churn, .pred_No) %>% 
  autoplot()
```
## Extract decision tree from the workflow

```{r}
final_tree <- extract_workflow(final_fit)
final_tree
```
## Visualizing decision tree from the workflow

```{r, warning=FALSE, message=FALSE}
library(rpart.plot)
final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
```

## Estimate variable importance

```{r, warning=FALSE, message=FALSE}
library(vip)

final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()
```

# Tune `rand_forest` hyperparameters

## Create model specification for tuning

```{r, echo=TRUE}
tune_spec <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) %>%
  set_engine("ranger") %>%
  set_mode("classification")

tune_spec 
```
## Create cross-validation folds for tuning

```{r, echo=TRUE}
set.seed(234)
vps_folds <- vfold_cv(vps_train)
vps_folds
```

## Initially fit models at all the different values

```{r, echo=TRUE}
set.seed(234)
doParallel::registerDoParallel()
tune_wf <- workflow() %>%
  add_recipe(vps_recipe) %>%
  add_model(tune_spec)

tune_res <- tune_grid(
  tune_wf,
  resamples = vps_folds,
  grid = 20
)

```

## Get metrics of different models

```{r}
tune_res %>% 
  collect_metrics()
```


## Plot initial tuning results 

```{r}
tune_res %>%
collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")
```

## Create detailed grid of hyperparameters values 

```{r, echo=TRUE}
rf_grid <- grid_regular(
  mtry(range = c(1, 3)),
  min_n(range = c(1, 5)),
  levels = 5
)

rf_grid
```


## Fit models at all the different values

```{r, echo=TRUE}
set.seed(456)
forest_res <- tune_wf %>%
  tune_grid(
  resamples = vps_folds,
  grid = rf_grid
)

```
## Get metrics of different models

```{r}
forest_res %>% 
  collect_metrics()
```
## Plot metrics of different models

```{r}
forest_res %>%
  collect_metrics() %>%
  mutate(mtry = factor(mtry)) %>%
  ggplot(aes(min_n, mean, color = mtry)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```


## Plot metrics of different models

```{r}
forest_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "AUC")
```

## Show models with best metrics

```{r}
forest_res %>%
  show_best("accuracy")
```


## Pick one model with the best metrics

```{r, echo=TRUE}
best_forest <- forest_res %>%
  select_best("accuracy")
best_forest
```



## Finalizing best model

```{r, echo=TRUE}
final_wf <- 
  tune_wf %>% 
  finalize_workflow(best_forest)

final_wf
```

## The final fit

```{r, echo=TRUE}
final_fit <- 
  final_wf %>%
  last_fit(vps_split) 

final_fit %>%
  collect_metrics()

```


## The final fit ROC curve

```{r}
final_fit %>%
  collect_predictions() %>% 
  roc_curve(is_churn, .pred_No) %>% 
  autoplot()
```

## The final fit Confusion Matrix

```{r}
final_fit %>%
  collect_predictions() %>% 
  conf_mat(is_churn, .pred_class) %>% 
  autoplot(type = "heatmap")
```



# Model deployment

```{r, echo=TRUE, warning=FALSE, message=FALSE, eval=FALSE}
production <- read_csv("./data/vps_test_data.txt")

tmp <- final_wf %>% fit(vps_train)  %>% predict(production) %>%   
  rename(is_churn = .pred_class) %>%
  mutate(is_churn = ifelse(is_churn == "Yes", 1, 0 ))

production %>% select(! one_of('is_churn')) %>%
  bind_cols(tmp) %>%
  write_csv("./data/vps_test_data_pred_part_3.txt")

```


# Further steps

- demonstrate how to use **SparkR** (R on Spark)

  