---
title: "VPS Customer churn prediction"
author: "JG Pardyak"
date: "19-8-2021"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## As an introduction

```{r, echo=FALSE, out.width="40%", fig.cap = "I did not expect that this interesting task would be intertwined with my long-awaited trip to Poland. So I start with Hello to everyone in front of the old locomotive in the city of Przeworsk. I tried my best to accomplish this task."}
knitr::include_graphics("./img/jacek.jpeg")
```


## Presentantion outline

![Cross Industry Standard Process for Data Mining (Wirth & Hipp, 2000)](./img/CRISP-DM_Process_Diagram.png)

## Business understanding

Company X sells a Virtual Private Server (VPS) as a service. The company wants to know which customers intend to leave VPS so they can devise an appropriate customer re-engagement strategy before it's too late.

## Data understanding

```{r, warning=FALSE, message=FALSE, echo=TRUE}
library(tidyverse)
library(tidymodels)
vps <- read_csv("./data/vps_churn_data.txt") %>%
  mutate(is_churn = factor(ifelse(is_churn == 0, 
                                  "No", "Yes")))

vps %>% dim()
```
In this work we use *R* and *tidy-* libraries. All commands are visible to facilitate the verification of the presentation. We see that the dataset is composed of 283 observations described with 23 variables.


## Data understanding cont.

The two classes (is_churn = Yes and is_churn = No) are almost equally distributed.

```{r, echo=TRUE}
vps %>% 
  count(is_churn) %>% 
  mutate(prop = n/sum(n))
```


## Data understanding cont.

The dataset is complete - there are no missing values we have to deal with.

```{r, echo=TRUE}
vps %>% is.na() %>% colSums() %>% head(6)
```

## Data understanding cont.

We check whether first variable can serve as a "good" predictor.

```{r}
vps %>%
  ggplot(aes(x = cpu_load_mean_m_3, fill = is_churn)) +
  geom_density(alpha = 0.3) 
```

## Data understanding cont.

We check whether last variable can serve as a "good" predictor.

```{r}
library(ggmosaic)
vps %>% 
  mutate(categor_network_tx_max_gradient = cut(network_tx_max_gradient, breaks=c(-Inf, 0, Inf),
                                       labels=c("negative","positive"))) %>%
  ggplot()  +
  geom_mosaic(aes(x = product( is_churn , categor_network_tx_max_gradient), fill = is_churn))
```

## Data preparation

We split our data into training and test datasets.

```{r, echo=TRUE}
set.seed(1234)
vps_split <- initial_split(vps, prop = 0.6, 
                           strata = is_churn)
vps_split
```

## Data preparation cont.

We check the observations used for training.

```{r}
vps_split %>%
  training() %>%
  glimpse()
```

## Data preparation cont.

We write recipe to prepare our data for training. Steps are described in comments.

```{r, echo=TRUE}
vps_recipe <- training(vps_split) %>% # on which data
  recipe(is_churn ~.) %>% # training formula
  step_rm(id) %>% # step remove id column
  # remove variables highly correlated with other vars
  step_corr(all_predictors()) %>% 
  # make vars to be of mean zero
  step_center(all_predictors(), -all_outcomes()) %>%
  # make vars to be standard dev of 1
  step_scale(all_predictors(), -all_outcomes()) %>% 
  prep() # execute transformations
```

## Data preparation cont.

We use previously written recipe to prepare *training* data.

```{r, echo=TRUE}
vps_training <- juice(vps_recipe)
vps_training %>% select(1:10) %>% glimpse()
```

## Data preparation cont.

We use previously written recipe to prepare *test* data.

```{r, echo=TRUE}
vps_testing <- vps_recipe %>%
  bake(testing(vps_split))
vps_testing %>% select(1:10) %>% glimpse()
```

## Modeling & Evaluation

Further we will interlace **Modeling** and **Evaluation** steps for selected algorithms. Previously defined *training* set is used to train models, the other *testing* for testing. Predicting power of each model is measured with *Accuracy* and *Area under curve* measures. 

## Modeling & Evaluation cont.

We start from *Null model* - assumption that no one will churn.

```{r, echo=TRUE}
null_model <- null_model(mode = "classification") %>% 
  set_engine("parsnip") %>%
  fit(is_churn ~ ., data = vps_training)
predict(null_model, vps_testing, type = "prob") %>%
  bind_cols(predict(null_model, vps_testing)) %>%
  bind_cols(select(vps_testing, is_churn)) %>%
  metrics(is_churn, .pred_No, estimate = .pred_class)
```

## Modeling & Evaluation cont.

From now on our goal is to beat 0.526 in accuracy, 0.5 in AUC and bend the ROC curve up.

```{r,  fig.width = 3}
null_model %>%
  predict(vps_testing, type = "prob") %>%
  bind_cols(vps_testing) %>%
  roc_curve(is_churn, .pred_No) %>%
  autoplot()
```

## Modeling & Evaluation cont.

We will try to train `svm_poly()` - polynomial support vector machines model.

```{r, echo=TRUE}
vps_svm <- svm_poly(mode = "classification") %>% 
  set_engine("kernlab") %>%
  fit(is_churn ~ ., data = vps_training)
```

## Modeling & Evaluation cont.

The `svm_poly()` model is performing worse than `null_model()`.

```{r, echo=TRUE}
predict(vps_svm, vps_testing, type = "prob") %>%
  bind_cols(predict(vps_svm, vps_testing)) %>%
  bind_cols(select(vps_testing, is_churn)) %>%
  metrics(is_churn, .pred_No, estimate = .pred_class)
```

## Modeling & Evaluation cont.

We will try to train `logistic_reg()` - generalized linear model for binary outcomes.

```{r, echo=TRUE}
vps_lreg <- logistic_reg(mode = "classification",
                         engine = "glm") %>% 
  fit(is_churn ~ ., data = vps_training)
```

## Modeling & Evaluation cont.

The `logistic_reg()` model is performing worse in accuracy but better in AUC than `null_model()`.

```{r, echo=TRUE}
predict(vps_lreg, vps_testing, type = "prob") %>%
  bind_cols(predict(vps_lreg, vps_testing)) %>%
  bind_cols(select(vps_testing, is_churn)) %>%
  metrics(is_churn, .pred_No, estimate = .pred_class)
```

## Modeling & Evaluation cont.

We will try to train `rand_forest()` - model that creates a large number of decision trees.

```{r, echo=TRUE}
vps_rf <-  rand_forest(mode = "classification") %>%
  set_engine("randomForest") %>%
  fit(is_churn ~ ., data = vps_training)
```

## Modeling & Evaluation cont.

The `rand_forest()` model is performing better in accuracy and AUC than `null_model()`.

```{r, echo=TRUE}
predict(vps_rf, vps_testing, type = "prob") %>%
  bind_cols(predict(vps_rf, vps_testing)) %>%
  bind_cols(select(vps_testing, is_churn)) %>%
  metrics(is_churn, .pred_No, estimate = .pred_class)
```

## Modeling & Evaluation cont.

Accuracy, AUC and ROC curve has been bent up comparing to null model.

```{r,  fig.width = 3}
vps_rf %>%
  predict(vps_testing, type = "prob") %>%
  bind_cols(vps_testing) %>%
  roc_curve(is_churn, .pred_No) %>%
  autoplot()
```

## Deployment

The last model will be used to make predictions on production data. Output is available in https://github.com/JacekPardyak/vps repository.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
production <- read_csv("./data/vps_test_data.txt")
vps_production <- vps_recipe %>%
  bake(production)

tmp <- predict(vps_rf, vps_production) %>% 
  rename(is_churn = .pred_class) %>%
  mutate(is_churn = ifelse(is_churn == "Yes", 1, 0 ))

production %>% select(! one_of('is_churn')) %>%
  bind_cols(tmp) %>%
  write_csv("./data/vps_test_data_pred.txt")

```


## Further steps

In other circumstances, I would go further with:

- feature engineering - combining and transforming further existing variables,

- tuning parameters of already tested algorithms used to train models,

- try another algorithms, such as: Boosted tree (XGBoost), K-nearest neighbor, Neural network with Keras

- demonstrate how to use **SparkR** (R on Spark)

  